


import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
import seaborn as sns
import string
import pandas_profiling
from nltk.corpus import stopwords
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import r2_score
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')
import re
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer





!pip install ydata-profiling


zomato_data_set = pd.read_csv('../../asset/zomato/zomato.csv')





zomato_data_set.head(1)


zomato_data_set.tail(1)





fresh_zomato_data = zomato_data_set.drop(['url','dish_liked','phone'], axis=1)





fresh_zomato_data.head(1)





total_of_duplicate =  fresh_zomato_data.duplicated().sum()
print(total_of_duplicate)





fresh_zomato_data.drop_duplicates(inplace=True)








fresh_zomato_data.isnull()





fresh_zomato_data.isnull().sum()





fresh_zomato_data.dropna(how='any', inplace=True)


fresh_zomato_data.isnull().sum()








fresh_zomato_data = fresh_zomato_data.rename(columns={'approx_cost(for two people)':'cost','listed_in(type)':'type', 'listed_in(city)':'city'})


fresh_zomato_data.tail(1)





fresh_zomato_data['cost'] = fresh_zomato_data['cost'].astype(str)
fresh_zomato_data['cost'] = fresh_zomato_data['cost'].apply(lambda x: x.replace(',','.'))
fresh_zomato_data['cost'] = fresh_zomato_data['cost'].astype(float)





fresh_zomato_data.head(1)





fresh_zomato_data = fresh_zomato_data.loc[fresh_zomato_data.rate !='NEW']
fresh_zomato_data = fresh_zomato_data.loc[fresh_zomato_data.rate !='-'].reset_index(drop=True)


fresh_zomato_data.tail(1)





remove_slash = lambda x: x.replace('/5', '') if isinstance(x, str) else str(x)
fresh_zomato_data.rate = fresh_zomato_data.rate.apply(remove_slash).str.strip().astype('float')


fresh_zomato_data.tail(1)








fresh_zomato_data.name = fresh_zomato_data.name.apply(lambda x:x.title())
fresh_zomato_data.online_order.replace(('Yes','No'),(True, False),inplace=True)
fresh_zomato_data.book_table.replace(('Yes','No'),(True, False),inplace=True)








restaurants = list(fresh_zomato_data['name'].unique())
fresh_zomato_data['Mean Rating'] = 0


fresh_zomato_data[['name', 'rate', 'Mean Rating']].tail(3)





for i in range(len(restaurants)):
    fresh_zomato_data['Mean Rating'][fresh_zomato_data['name'] == restaurants[i]] = fresh_zomato_data['rate'][fresh_zomato_data['name'] == restaurants[i]].mean()


pai_vihar_data = fresh_zomato_data[fresh_zomato_data['name'] == 'Pai Vihar']
pai_vihar_data[['name', 'rate', 'Mean Rating']]








scaler = MinMaxScaler(feature_range = (1,5))
fresh_zomato_data[['Mean Rating']] = scaler.fit_transform(fresh_zomato_data[['Mean Rating']]).round(2)








fresh_zomato_data["reviews_list"] = fresh_zomato_data["reviews_list"].str.lower()








PUNCT_TO_REMOVE = string.punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

fresh_zomato_data["reviews_list"] = fresh_zomato_data["reviews_list"].apply(lambda text: remove_punctuation(text))





STOPWORDS = set(stopwords.words('english'))
def remove_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

fresh_zomato_data["reviews_list"] = fresh_zomato_data["reviews_list"].apply(lambda text: remove_stopwords(text))





def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

fresh_zomato_data["reviews_list"] = fresh_zomato_data["reviews_list"].apply(lambda text: remove_urls(text))


fresh_zomato_data[['reviews_list', 'cuisines']].sample(5)


restaurant_names = list(fresh_zomato_data['name'].unique())
def get_top_words(column, top_nu_of_words, nu_of_word):
    vec = CountVectorizer(ngram_range= nu_of_word, stop_words='english')
    bag_of_words = vec.fit_transform(column)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:top_nu_of_words]
    
fresh_zomato_data=fresh_zomato_data.drop(['address','rest_type', 'type', 'menu_item', 'votes'],axis=1)


df_percent = fresh_zomato_data.sample(frac=0.5)
df_percent


df_percent.set_index('name', inplace=True)
indices = pd.Series(df_percent.index)

# Creating tf-idf matrix
tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0.01, stop_words='english')
tfidf_matrix = tfidf.fit_transform(df_percent['reviews_list'])

cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)


def recommend(name, cosine_similarities=cosine_similarities):
    # Create a list to put top restaurants
    recommend_restaurant = []
    
    # Find the index of the hotel entered
    idx = indices[indices == name].index[0]
    
    # Find the restaurants with a similar cosine-sim value and order them from biggest number
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False)
    
    # Extract top 30 restaurant indexes with a similar cosine-sim value
    top30_indexes = list(score_series.iloc[0:31].index)
    
    # Names of the top 30 restaurants
    for each in top30_indexes:
        recommend_restaurant.append(list(df_percent.index)[each])
    
    # Creating the new data set to show similar restaurants
    df_new = pd.DataFrame(columns=['cuisines', 'Mean Rating', 'cost'])
    
    # Create a list to store DataFrames for concatenation
    frames = []
    for each in recommend_restaurant:
        frame = pd.DataFrame(df_percent[['cuisines', 'Mean Rating', 'cost']][df_percent.index == each].sample())
        frames.append(frame)
    
    # Concatenate all DataFrames
    df_new = pd.concat(frames, ignore_index=True)
    
    # Drop the same named restaurants and sort only the top 10 by the highest rating
    df_new = df_new.drop_duplicates(subset=['cuisines', 'Mean Rating', 'cost'], keep=False)
    df_new = df_new.sort_values(by='Mean Rating', ascending=False).head(10)
    
    print('TOP %s RESTAURANTS LIKE %s WITH SIMILAR REVIEWS: ' % (str(len(df_new)), name))

    # Inside the recommend function, after df_new is created:
    # Bar Chart for Mean Rating
    plt.figure(figsize=(8, 6))
    plt.bar(df_new.index, df_new['Mean Rating'], color=['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF', '#FF9F40', '#66CC66', '#FF99CC', '#33CCCC', '#CC99FF'])
    plt.xlabel('Restaurant Index')
    plt.ylabel('Mean Rating (1-5)')
    plt.title('Mean Ratings of Top 10 Recommended Restaurants')
    plt.show()
    
    # Scatter Chart for Mean Rating vs Cost
    plt.figure(figsize=(8, 6))
    plt.scatter(df_new['cost'], df_new['Mean Rating'], color='#FF6384')
    plt.xlabel('Cost (INR)')
    plt.ylabel('Mean Rating (1-5)')
    plt.title('Mean Rating vs Cost of Top 10 Recommended Restaurants')
    plt.show()
    
    # Pie Chart for Cuisine Frequency
    cuisine_counts = {}
    for cuisines in df_new['cuisines']:
        for cuisine in cuisines.split(','):
            cuisine = cuisine.strip()
            cuisine_counts[cuisine] = cuisine_counts.get(cuisine, 0) + 1
    plt.figure(figsize=(8, 6))
    plt.pie(cuisine_counts.values(), labels=cuisine_counts.keys(), autopct='%1.1f%%', colors=['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF', '#FF9F40', '#66CC66', '#FF99CC', '#33CCCC', '#CC99FF'])
    plt.title('Cuisine Frequency in Top 10 Recommended Restaurants')
    plt.show()

    # pandas_profiling.ProfileReport(df_new)
    return df_new


new_df = restaurant_names[1:6]


new_df


df2 =  recommend(new_df[1])


pandas_profiling.ProfileReport(df2)


pandas_profiling.ProfileReport(recommend(new_df[3]))


pandas_profiling.ProfileReport(recommend(new_df[4]))



